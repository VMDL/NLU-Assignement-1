{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNokgLkSpZHXIoUFW1MlTGy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cPD9rX98KptQ"},"source":["# **Util functions**\n"]},{"cell_type":"markdown","metadata":{"id":"2y5OjBPvK8BJ"},"source":["The function *get_doc* is a function which takes as input a string representing a sentence and it returns as output a [*Doc*](https://spacy.io/api/doc).\n","\n","A *Doc* is made up of a sequence of [*Token*](https://spacy.io/api/token).\n","\n","Before to get the *Doc* instance, it is necessary to load a [*Language*](https://spacy.io/api/language/) through the method [*load*](https://spacy.io/usage/processing-pipelines).\n"]},{"cell_type":"code","metadata":{"id":"U0ifRYqfKnwO"},"source":["import spacy\n","def get_doc(sentence):\n","  spacy_nlp = spacy.load('en_core_web_sm')\n","  return spacy_nlp(sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjGslDlHGtxj"},"source":["# **Exercise 1** "]},{"cell_type":"markdown","metadata":{"id":"-ntcP9xYGt6f"},"source":["Exercise: *Extract a path of dependency relations from the ROOT to a token*\n","\n","The function *path_of_dependencies* takes a string representing a sentence as input and return a list of list representing the path of dependency relations from the ROOT to a token for each token in the sentence.\n","\n","The first necessary operation is to load the *Doc* instance by exploiting the function *get_doc* described in the previous section.\n","\n","Then for each token in the sentence it extracts, it appends to the current list *tmp_list* the dependency relation of the current token by extracting its attribute [*dep_*](https://spacy.io/api/token#attributes), then it iterates from the token under analysis to the root by resorting to the attribute [*head*](https://spacy.io/usage/linguistic-features#navigating) of a token and for each token along the path it push the dependency relation of token crossed, after having reached the [*ROOT*](https://spacy.io/usage/linguistic-features#pos-tagging), it appends the entire current list to the list of list previously defined.\n","\n","After having repeated this iterative cycle for all tokens, it return the list of list.\n","\n","The reason behind the choice of a list of list instead of a dictionary of list is described in detail the \"Report.pdf\" file.\n","\n"]},{"cell_type":"code","metadata":{"id":"yGRBXfyQKZ4_"},"source":["def path_of_dependencies(sentence):\n","  spacy_doc = get_doc(sentence)\n","  list_of_paths = []\n","  for token in spacy_doc:\n","    tmp_list = []\n","    tmp_list.append(token.dep_)\n","    while token.head != token:\n","      token = token.head\n","      tmp_list.insert(0,token.dep_)\n","    list_of_paths.append(tmp_list)\n","  return list_of_paths"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JvpVN4c8Gv_y"},"source":["# **Exercise 2**"]},{"cell_type":"markdown","metadata":{"id":"9UdriVWCIXH-"},"source":["Exercise: *Extract subtree of a dependents given a token*\n","\n","The function *sorted_path_of_dependencies* takes as input a string representing a sentence and it returns as output a list of list.\n","\n","The first operation is to load the *Doc* instance by using the function *get_doc* described in the \"Util function\" section.\n","\n","Then for each token in the Doc, it exploit the attribute *subtree* of the current Token which returns a generator which is converted into a list by using the function *list* and this list is then appended to the list of all paths.\n","\n","After having repeated this cycle over all tokens in the Doc, it returns the list of all paths.\n","\n","The reason behind the choice of a list of list instead of a dictionary of list is described in detail the \"Report.pdf\" file.\n","\n"]},{"cell_type":"code","metadata":{"id":"LZchn8jHGy6q"},"source":["def sorted_list_of_dependencies(sentence):\n","  spacy_doc = get_doc(sentence)\n","  sorted_list_of_paths = []\n","  for token in spacy_doc:\n","    sorted_list_of_paths.append(list(token.subtree))\n","  return sorted_list_of_paths\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bcNQqm1vJgHk"},"source":["# **Exercise 3**"]},{"cell_type":"markdown","metadata":{"id":"pAv_dSrmPMay"},"source":["Exercise: *check if a given list of tokens (segment of a sentence) forms a subtree*\n","\n","The function *seq_is_subtree* verifies if a sequence of tokens (described by the second argument *list_of_words*) represent a subtree of a string representing a sentence which is described by the first argument *sentence* and it returns as output a Boolean value.\n","\n","As in the previous exercise, the first mandatory operation is to define the processed Doc by calling the *get_doc* function.\n","\n","Then for each token in the Doc, it uses the attribute *subtree* of the current token, in order to return its subtree which is made up of generator of Token instances, then in order to convert this object into a string, I have defined the *gen_to_str* function which takes the subtree as input and convert each token into a string which is then appended into the list of strings which will be returned the *gen_to_str* function, then we verify if the list of strings returned by *gen_to_str* is equal to the list of tokens passed as input to the function *seq_is_subtree* by using the equality operator (*==*), if they are equal, the function returns *True* otherwise *False*"]},{"cell_type":"code","metadata":{"id":"mU9hnYCnPMqm"},"source":["def gen_to_str(subtree):\n","    return [(str(it)) for it in subtree]\n","\n","\n","def seq_is_subtree(sentence, list_of_tokens):\n","  spacy_doc = get_doc(sentence)\n","  for token in spacy_doc:\n","    str_subtree = gen_to_str(token.subtree)\n","    if str_subtree == list_of_tokens:\n","      return True\n","  return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Go0sDplwJkd-"},"source":["# **Exercise 4**"]},{"cell_type":"markdown","metadata":{"id":"DgiZed61EOqZ"},"source":["Exercise: *identify head of a span, given its tokens*\n","\n","The function *get_head* receives an input a string representing a *sentence* and  returns as output a  Token representing the head of the input sentece.\n","\n","As in the  previous  exercises,  the  first operation  to  perform  is  the  Doc  processing by exploiting the *get_doc* function.\n","\n","Then in  order to return the root, we need to store in a local variable the Span (list of tokens) contained in the Doc instance and finally return the [*root*](https://spacy.io/api/span#root) attribute of the [Span](https://spacy.io/api/span) instance representing the input sentence.\n"]},{"cell_type":"code","metadata":{"id":"eilrqxNtEPI_"},"source":["def get_head(sentence):\n","  doc = get_doc(sentence)\n","  span = doc[:]\n","  return span.root"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iQuqNp5bFCbE"},"source":["# **Exercise 5**"]},{"cell_type":"markdown","metadata":{"id":"PrUC_HokFIdy"},"source":["Exercise: *Extract sentence subject, direct object and indirect object spans*\n","\n","The function *extract_soi* receives as input a string representing the input sentence and returns a dictionary of lists as output.\n","\n","The first thing to do is to define the processed Doc by using the *get_doc* function, then we need to declare a dictionary which, we will use to store the token of interest, made up of three different keys:\n","1. [*subj*](https://spacy.io/models/en#en_core_web_sm) representing the subjects\n","2. [*dobj*](https://spacy.io/models/en#en_core_web_sm) representing the direct objects\n","3. [*dative*](https://spacy.io/models/en#en_core_web_sm) representing the indirect objects\n","\n","After this declaration, we have to iterate over all tokens in the input sentence\n","and check their attribute [*dep*](https://spacy.io/api/token#attributes) telling us the dependency relation.\n","\n","We need to check if the current token is a subject, a direct object or an indirect object, if it is one of them, we append the current token to the list corresponding to that specific dependency relation defined by the *key*s previously described of the dictionary.\n","\n","Finally, after having repeated this process for all tokens, we can return the dictionary of lists."]},{"cell_type":"code","metadata":{"id":"2e52gSM4FIlX"},"source":["from collections import defaultdict\n","\n","#soi represents \"subject\", \"direct object\" and \"indirect object\"\n","def extract_soi(sentence, look_for = ['nsubj','dobj','dative']):\n","  doc = get_doc(sentence)\n","  dict_of_spans = defaultdict(list)\n","  for it in look_for:\n","    dict_of_spans[it] = []\n","  for token in doc:\n","    for it in look_for:\n","      if token.dep_ == it:\n","        dict_of_spans[it].append(token)\n","  return dict_of_spans\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h1mafjaqJiDL"},"source":["#**Test**"]},{"cell_type":"markdown","metadata":{"id":"R3x0Qd5PNHgP"},"source":["*First exercise*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4gQ3qXmNE-8","executionInfo":{"status":"ok","timestamp":1618889681422,"user_tz":-120,"elapsed":2401,"user":{"displayName":"Marco De Luca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4bvlddv9S2r5WXxd3HMPzGFo0gE5n6qyWYHYpNg=s64","userId":"11570652135950771421"}},"outputId":"c39572d3-0aad-49a8-8a55-355685e5eea7"},"source":["sentence = \"I saw a man with a telescope .\"\n","sentence2 = \"The SMITH model produced by Google outperforms the BERT model\"\n","sentence3 = \"The Transformer mechanism works better with long distance sequences than Convolutional Neural Networks\"\n","sentence4 = \"The famous linguist Noam Chomsky was born in 1928 in Philadelphia\"\n","\n","\n","list_of_paths = path_of_dependencies(sentence)\n","\n","doc = get_doc(sentence)\n","for idx, it_path in enumerate(list_of_paths):\n","  print(\"token:{} - path:{}\".format(doc[idx].text, it_path))"],"execution_count":100,"outputs":[{"output_type":"stream","text":["token:I - path:['ROOT', 'nsubj']\n","token:saw - path:['ROOT']\n","token:a - path:['ROOT', 'dobj', 'det']\n","token:man - path:['ROOT', 'dobj']\n","token:with - path:['ROOT', 'dobj', 'prep']\n","token:a - path:['ROOT', 'dobj', 'prep', 'pobj', 'det']\n","token:telescope - path:['ROOT', 'dobj', 'prep', 'pobj']\n","token:. - path:['ROOT', 'punct']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zwin34TdOTud"},"source":["*Second exercise*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wck0soZcOXu9","executionInfo":{"status":"ok","timestamp":1618889701361,"user_tz":-120,"elapsed":1639,"user":{"displayName":"Marco De Luca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4bvlddv9S2r5WXxd3HMPzGFo0gE5n6qyWYHYpNg=s64","userId":"11570652135950771421"}},"outputId":"8312a4bb-aeca-45c6-fbef-59948d2f1680"},"source":["sorted_list_of_paths = sorted_list_of_dependencies(sentence)\n","\n","for it in sorted_list_of_paths:\n","  print(\"token:{} - sorted path:{}\".format(it[0], it))"],"execution_count":101,"outputs":[{"output_type":"stream","text":["token:I - sorted path:[I]\n","token:I - sorted path:[I, saw, a, man, with, a, telescope, .]\n","token:a - sorted path:[a]\n","token:a - sorted path:[a, man, with, a, telescope]\n","token:with - sorted path:[with, a, telescope]\n","token:a - sorted path:[a]\n","token:a - sorted path:[a, telescope]\n","token:. - sorted path:[.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G7DzNMe6PZnX"},"source":["*Third exercise*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1pS5fTVPctm","executionInfo":{"status":"ok","timestamp":1618889764494,"user_tz":-120,"elapsed":7926,"user":{"displayName":"Marco De Luca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4bvlddv9S2r5WXxd3HMPzGFo0gE5n6qyWYHYpNg=s64","userId":"11570652135950771421"}},"outputId":"7c62531b-3421-4bd3-c594-f628d40fd161"},"source":["list_of_tokens = [[\"saw\",\"man\",\"with\"],[\"with\",\"a\",\"telescope\"],[\"a\"],[\"I\",\"saw\"]]\n","\n","for it in list_of_tokens:\n","  bool_answ = seq_is_subtree(sentence, it)\n","  if bool_answ == True:\n","    print(\"The list of tokens: {} is a subtree of the sentence {}\".format(it, sentence))\n","  else:\n","    print(\"The list of tokens: {} is not a subtree of the sentence {}\".format(it, sentence))"],"execution_count":104,"outputs":[{"output_type":"stream","text":["The list of tokens: ['saw', 'man', 'with'] is not a subtree of the sentence I saw a man with a telescope .\n","The list of tokens: ['with', 'a', 'telescope'] is a subtree of the sentence I saw a man with a telescope .\n","The list of tokens: ['a'] is a subtree of the sentence I saw a man with a telescope .\n","The list of tokens: ['I', 'saw'] is not a subtree of the sentence I saw a man with a telescope .\n","Is ['saw', 'man', 'with'] a subtree: False\n","Is ['with', 'a', 'telescope'] a subtree: True\n","Is ['a'] a subtree: True\n","Is ['I', 'saw'] a subtree: False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NV5r5awxEmTH"},"source":["*Forth exercise*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MxvTgIFDEmn5","executionInfo":{"status":"ok","timestamp":1618888998786,"user_tz":-120,"elapsed":3256,"user":{"displayName":"Marco De Luca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4bvlddv9S2r5WXxd3HMPzGFo0gE5n6qyWYHYpNg=s64","userId":"11570652135950771421"}},"outputId":"9e96ac65-37d0-4a1b-cd99-a0020e087e56"},"source":["print(\"The root of: '{}' is '{}'\".format(sentence, get_head(sentence)))"],"execution_count":78,"outputs":[{"output_type":"stream","text":["The root of: 'I saw a man with a telescope.' is 'saw'\n","Head of \"I saw a man with a telescope.\": saw\n","Head of \"The quick brown fox jumps over the lazy dog.\": jumps\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_gIo9utWE8Rc"},"source":["Fifth exercise"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4SAHBdPcE-QI","executionInfo":{"status":"ok","timestamp":1618882563085,"user_tz":-120,"elapsed":60375,"user":{"displayName":"Marco De Luca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4bvlddv9S2r5WXxd3HMPzGFo0gE5n6qyWYHYpNg=s64","userId":"11570652135950771421"}},"outputId":"56b45441-d13a-487a-9093-d3d71871f7f6"},"source":["dict_soi = extract_soi(sentence)\n","\n","print(\"The first sentence is: {}\".format(sentence))\n","for it in dict_soi:\n","  print(\"{}: {}\".format(it, dict_soi[it]))\n","\n","\n","\n","sentence2=\"Joe gave Jim the ball\"\n","dict_soi = extract_soi(sentence2)\n","\n","print(\"The second sentence is: {}\".format(sentence2))\n","for it in dict_soi:\n","  print(\"{}: {}\".format(it, dict_soi[it]))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The first sentence is: I saw a man with a telescope.\n","nsubj: [I]\n","dobj: [man]\n","dative: []\n","The second sentence is: Joe gave Jim the ball\n","nsubj: [Joe]\n","dobj: [ball]\n","dative: [Jim]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lHwsrAiH5aF_"},"source":["# **Advanced and Optional part**"]},{"cell_type":"markdown","metadata":{"id":"wLZZXcOH5l1q"},"source":["Exercise 1: *Modify NLTK Transition parser's Configuration class to use better features*\n","\n","In order to improve the model performance, I have added the following features to the result string in the function *extract_features*:\n"," the Levenshtein distance between the word on the topof the stack and the \n","\n","1.   Levenshtein distance between the word on the top of the stack and the last word in the buffer\n","\n","2.   The left child of the first node in the buffer\n","\n","3.   The right child of the first node in the buffer\n","\n","4.  The index of the element in the buffer taken in consideration\n","\n","5.  The lenght of the buffer\n","\n","These features added have been inspired by the [PAPER], the Stanford's Natural Language Processing course and some empirical tries.\n","\n","In order to add these features to the original ones, it is required to specify the third argument of the *train* function (*modified_features=True*) and the third argument of the *parse* function (*modified_features=True*).\n","\n","N.B. Modified features is set to *False* as default value.\n","\n","Exercise 2: *Evaluate the features comparing performance to the original*\n","\n","In order to evaluate the features, it is required to train and test the model by specifying the third argument of the train function and the third argument of parse function (*modified_features*) as *True* or *False* \n","\n","Exercise 3:*Replace SVM classifier with an alternative of your choice*\n","\n","In the *Train* function, I have added an additional parameter (*model_spec*) in order to specify the model to be used during training.\n","\n","The default value is (*spec_mod='svm'*) in order to use the SVM model.\n","\n","The other model supported (and which has been tested) are:\n","\n","1.   Decision Tree Classifier (*spec_mod = 'DecisionTreeClassifier'*)\n","2.   Random Forest (*spec_mod = 'RandomForestClassifier'*)\n","3.   Multi-Layer Perceptron (*spec_mod = 'MLPClassifier'*)\n","\n","The model which provides the best performance has been the *Decision Tree Classifier* which guarantees results pretty similar to the SVM but in much less training time.\n","\n","For detailed analysis about the performance of the different models (by using the original features or the modified features) refer to the file 'Report.pdf'.\n","\n","NB: In order to simplify the reading of the added part with respect to the original Configuration class, I have added as comments: \"#BEGIN ADDED CODE pt.\" at the beginning of the additional line of codes and \"#END ADDED CODE pt.\" at the end, while for the modification in the argument passed to the function I have added as comment:\"#THE ARGUMENTS ARE CHANGED\"\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Ig9s8Fgz6CV0","executionInfo":{"status":"ok","timestamp":1618883786584,"user_tz":-120,"elapsed":3667,"user":{"displayName":"Marco De Luca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4bvlddv9S2r5WXxd3HMPzGFo0gE5n6qyWYHYpNg=s64","userId":"11570652135950771421"}}},"source":["# Natural Language Toolkit: Arc-Standard and Arc-eager Transition Based Parsers\n","import tempfile\n","import pickle\n","\n","from os import remove\n","from copy import deepcopy\n","from operator import itemgetter\n","\n","try:\n","    from numpy import array\n","    from scipy import sparse\n","    from sklearn.datasets import load_svmlight_file\n","    from sklearn import svm\n","    from sklearn.tree import DecisionTreeClassifier\n","    from sklearn.ensemble import RandomForestClassifier\n","except ImportError:\n","    pass\n","\n","from nltk.parse import ParserI, DependencyGraph, DependencyEvaluator\n","\n","\n","class Configuration(object):\n","    \"\"\"\n","    Class for holding configuration which is the partial analysis of the input sentence.\n","    The transition based parser aims at finding set of operators that transfer the initial\n","    configuration to the terminal configuration.\n","    The configuration includes:\n","        - Stack: for storing partially proceeded words\n","        - Buffer: for storing remaining input words\n","        - Set of arcs: for storing partially built dependency tree\n","    This class also provides a method to represent a configuration as list of features.\n","    \"\"\"\n","\n","    def __init__(self, dep_graph):\n","        \"\"\"\n","        :param dep_graph: the representation of an input in the form of dependency graph.\n","        :type dep_graph: DependencyGraph where the dependencies are not specified.\n","        \"\"\"\n","        # dep_graph.nodes contain list of token for a sentence\n","        self.stack = [0]  # The root element\n","        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n","        self.arcs = []  # empty set of arc\n","        self._tokens = dep_graph.nodes\n","        self._max_address = len(self.buffer)\n","        #BEGIN ADDED CODE pt.0\n","        self.dg = dep_graph\n","        #END ADDED CODE pt.0      \n","\n","    def __str__(self):\n","        return (\n","            \"Stack : \"\n","            + str(self.stack)\n","            + \"  Buffer : \"\n","            + str(self.buffer)\n","            + \"   Arcs : \"\n","            + str(self.arcs)\n","        )\n","\n","    def _check_informative(self, feat, flag=False):\n","        \"\"\"\n","        Check whether a feature is informative\n","        The flag control whether \"_\" is informative or not\n","        \"\"\"\n","        if feat is None:\n","            return False\n","        if feat == \"\":\n","            return False\n","        if flag is False:\n","            if feat == \"_\":\n","                return False\n","        return True\n","\n","    #THE ARGUMENTS ARE CHANGED\n","    def extract_features(self, modified_features=False):\n","        result = []\n","\n","        #BEGIN ADDED CODE pt.1\n","        stack_top_token = None\n","        stack_idx = None\n","        #END ADDED CODE pt.1\n","\n","        if len(self.stack) > 0:\n","            # Stack 0\n","            stack_idx0 = self.stack[len(self.stack) - 1]\n","            token = self._tokens[stack_idx0]\n","            #BEGIN ADDED CODE pt.2\n","            stack_top_token = token[\"word\"]   \n","            stack_idx = stack_idx0\n","            #END ADDED CODE pt.2\n","            if self._check_informative(token[\"word\"], True):\n","                result.append(\"STK_0_FORM_\" + token[\"word\"])\n","            if \"lemma\" in token and self._check_informative(token[\"lemma\"]):\n","                result.append(\"STK_0_LEMMA_\" + token[\"lemma\"])\n","            if self._check_informative(token[\"tag\"]):\n","                result.append(\"STK_0_POS_\" + token[\"tag\"])\n","            if \"feats\" in token and self._check_informative(token[\"feats\"]):\n","                feats = token[\"feats\"].split(\"|\")\n","                for feat in feats:\n","                    result.append(\"STK_0_FEATS_\" + feat)\n","            # Stack 1\n","            if len(self.stack) > 1:\n","                stack_idx1 = self.stack[len(self.stack) - 2]\n","                token = self._tokens[stack_idx1]\n","                if self._check_informative(token[\"tag\"]):\n","                    result.append(\"STK_1_POS_\" + token[\"tag\"])\n","\n","            # Left most, right most dependency of stack[0]\n","            left_most = 1000000\n","            right_most = -1\n","            dep_left_most = \"\"\n","            dep_right_most = \"\"\n","            for (wi, r, wj) in self.arcs:\n","                if wi == stack_idx0:\n","                    if (wj > wi) and (wj > right_most):\n","                        right_most = wj\n","                        dep_right_most = r\n","                    if (wj < wi) and (wj < left_most):\n","                        left_most = wj\n","                        dep_left_most = r\n","            if self._check_informative(dep_left_most):\n","                result.append(\"STK_0_LDEP_\" + dep_left_most)\n","            if self._check_informative(dep_right_most):\n","                result.append(\"STK_0_RDEP_\" + dep_right_most)\n","\n","        # Check Buffered 0\n","\n","        #BEGIN ADDED CODE pt.3\n","        buffer_first_token = None\n","        buffer_idx = None\n","        #END ADDED CODE pt.3\n","\n","        if len(self.buffer) > 0:\n","            # Buffer 0\n","            buffer_idx0 = self.buffer[0]\n","            token = self._tokens[buffer_idx0]\n","            #BEGIN ADDED CODE pt.4\n","            buffer_idx = buffer_idx0\n","            buffer_first_token = token[\"word\"]\n","            #END ADDED CODE pt.4\n","            if self._check_informative(token[\"word\"], True):\n","                result.append(\"BUF_0_FORM_\" + token[\"word\"])\n","            if \"lemma\" in token and self._check_informative(token[\"lemma\"]):\n","                result.append(\"BUF_0_LEMMA_\" + token[\"lemma\"])\n","            if self._check_informative(token[\"tag\"]):\n","                result.append(\"BUF_0_POS_\" + token[\"tag\"])\n","            if \"feats\" in token and self._check_informative(token[\"feats\"]):\n","                feats = token[\"feats\"].split(\"|\")\n","                for feat in feats:\n","                    result.append(\"BUF_0_FEATS_\" + feat)\n","            # Buffer 1\n","            if len(self.buffer) > 1:\n","                buffer_idx1 = self.buffer[1]\n","                token = self._tokens[buffer_idx1]            \n","                if self._check_informative(token[\"word\"], True):\n","                    result.append(\"BUF_1_FORM_\" + token[\"word\"])\n","                if self._check_informative(token[\"tag\"]):\n","                    result.append(\"BUF_1_POS_\" + token[\"tag\"])\n","            if len(self.buffer) > 2:\n","                buffer_idx2 = self.buffer[2]\n","                token = self._tokens[buffer_idx2]    \n","                if self._check_informative(token[\"tag\"]):\n","                    result.append(\"BUF_2_POS_\" + token[\"tag\"])\n","            if len(self.buffer) > 3:\n","                buffer_idx3 = self.buffer[3]\n","                token = self._tokens[buffer_idx3]   \n","                if self._check_informative(token[\"tag\"]):\n","                    result.append(\"BUF_3_POS_\" + token[\"tag\"])\n","                    # Left most, right most dependency of stack[0]\n","            left_most = 1000000\n","            right_most = -1\n","            dep_left_most = \"\"\n","            dep_right_most = \"\"\n","            for (wi, r, wj) in self.arcs:\n","                if wi == buffer_idx0:\n","                    if (wj > wi) and (wj > right_most):\n","                        right_most = wj\n","                        dep_right_most = r\n","                    if (wj < wi) and (wj < left_most):\n","                        left_most = wj\n","                        dep_left_most = r\n","            if self._check_informative(dep_left_most):\n","                result.append(\"BUF_0_LDEP_\" + dep_left_most)\n","            if self._check_informative(dep_right_most):\n","                result.append(\"BUF_0_RDEP_\" + dep_right_most)\n","        #BEGIN ADDED CODE pt.5  \n","        if modified_features == True:\n","          #Compute distance\n","          from pytextdist.edit_distance import levenshtein_distance\n","          if buffer_first_token is not None and stack_top_token is not None:\n","            result.append(\"DIS_\"+str(levenshtein_distance(buffer_first_token, stack_top_token)))\n","          else:\n","            result.append(\"DIS_\"+str(0))\n","          if buffer_idx is not None:\n","            result.append(\"BI\"+str(buffer_idx))\n","            result.append(\"BLC\"+str(self.dg.left_children(buffer_idx)))\n","            result.append(\"BRC\"+str(self.dg.right_children(buffer_idx)))\n","          else:\n","            result.append(\"BI\"+str(0))\n","            result.append(\"BLC\"+str(0))\n","            result.append(\"BRC\"+str(0))\n","          result.append(\"BUFF_LEN\"+str(len(self.buffer)))\n","          #END ADDED CODE pt.5\n","          \n","          #if stack_idx is not None:\n","          #  result.append(\"SI\"+str(stack_idx))\n","          #  result.append(\"SLC\"+str(self.dg.left_children(stack_idx)))\n","          #  result.append(\"SRC\"+str(self.dg.right_children(stack_idx)))\n","          #else:\n","          #  result.append(\"SI\"+str(0))\n","          #  result.append(\"SLC\"+str(0))\n","          #  result.append(\"SRC\"+str(0))\n","          #result.append(\"STACK LEN\"+str(len(self.stack)))\n","        return result\n","\n","\n","class Transition(object):\n","    \"\"\"\n","    This class defines a set of transition which is applied to a configuration to get another configuration\n","    Note that for different parsing algorithm, the transition is different.\n","    \"\"\"\n","\n","    # Define set of transitions\n","    LEFT_ARC = \"LEFTARC\"\n","    RIGHT_ARC = \"RIGHTARC\"\n","    SHIFT = \"SHIFT\"\n","    REDUCE = \"REDUCE\"\n","\n","    def __init__(self, alg_option):\n","        \"\"\"\n","        :param alg_option: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n","        :type alg_option: str\n","        \"\"\"\n","        self._algo = alg_option\n","        if alg_option not in [\n","            TransitionParser.ARC_STANDARD,\n","            TransitionParser.ARC_EAGER,\n","        ]:\n","            raise ValueError(\n","                \" Currently we only support %s and %s \"\n","                % (TransitionParser.ARC_STANDARD, TransitionParser.ARC_EAGER)\n","            )\n","\n","    def left_arc(self, conf, relation):\n","        \"\"\"\n","        Note that the algorithm for left-arc is quite similar except for precondition for both arc-standard and arc-eager\n","            :param configuration: is the current configuration\n","            :return : A new configuration or -1 if the pre-condition is not satisfied\n","        \"\"\"\n","        if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n","            return -1\n","        if conf.buffer[0] == 0:\n","            # here is the Root element\n","            return -1\n","\n","        idx_wi = conf.stack[len(conf.stack) - 1]\n","\n","        flag = True\n","        if self._algo == TransitionParser.ARC_EAGER:\n","            for (idx_parent, r, idx_child) in conf.arcs:\n","                if idx_child == idx_wi:\n","                    flag = False\n","\n","        if flag:\n","            conf.stack.pop()\n","            idx_wj = conf.buffer[0]\n","            conf.arcs.append((idx_wj, relation, idx_wi))\n","        else:\n","            return -1\n","\n","    def right_arc(self, conf, relation):\n","        \"\"\"\n","        Note that the algorithm for right-arc is DIFFERENT for arc-standard and arc-eager\n","            :param configuration: is the current configuration\n","            :return : A new configuration or -1 if the pre-condition is not satisfied\n","        \"\"\"\n","        if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n","            return -1\n","        if self._algo == TransitionParser.ARC_STANDARD:\n","            idx_wi = conf.stack.pop()\n","            idx_wj = conf.buffer[0]\n","            conf.buffer[0] = idx_wi\n","            conf.arcs.append((idx_wi, relation, idx_wj))\n","        else:  # arc-eager\n","            idx_wi = conf.stack[len(conf.stack) - 1]\n","            idx_wj = conf.buffer.pop(0)\n","            conf.stack.append(idx_wj)\n","            conf.arcs.append((idx_wi, relation, idx_wj))\n","\n","    def reduce(self, conf):\n","        \"\"\"\n","        Note that the algorithm for reduce is only available for arc-eager\n","            :param configuration: is the current configuration\n","            :return : A new configuration or -1 if the pre-condition is not satisfied\n","        \"\"\"\n","\n","        if self._algo != TransitionParser.ARC_EAGER:\n","            return -1\n","        if len(conf.stack) <= 0:\n","            return -1\n","\n","        idx_wi = conf.stack[len(conf.stack) - 1]\n","        flag = False\n","        for (idx_parent, r, idx_child) in conf.arcs:\n","            if idx_child == idx_wi:\n","                flag = True\n","        if flag:\n","            conf.stack.pop()  # reduce it\n","        else:\n","            return -1\n","\n","    def shift(self, conf):\n","        \"\"\"\n","        Note that the algorithm for shift is the SAME for arc-standard and arc-eager\n","            :param configuration: is the current configuration\n","            :return : A new configuration or -1 if the pre-condition is not satisfied\n","        \"\"\"\n","        if len(conf.buffer) <= 0:\n","            return -1\n","        idx_wi = conf.buffer.pop(0)\n","        conf.stack.append(idx_wi)\n","\n","\n","class TransitionParser(ParserI):\n","\n","    \"\"\"\n","    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n","    \"\"\"\n","\n","    ARC_STANDARD = \"arc-standard\"\n","    ARC_EAGER = \"arc-eager\"\n","\n","    def __init__(self, algorithm):\n","        \"\"\"\n","        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n","        :type algorithm: str\n","        \"\"\"\n","        if not (algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n","            raise ValueError(\n","                \" Currently we only support %s and %s \"\n","                % (self.ARC_STANDARD, self.ARC_EAGER)\n","            )\n","        self._algorithm = algorithm\n","\n","        self._dictionary = {}\n","        self._transition = {}\n","        self._match_transition = {}\n","\n","    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n","        p_node = depgraph.nodes[idx_parent]\n","        c_node = depgraph.nodes[idx_child]\n","\n","        if c_node[\"word\"] is None:\n","            return None  # Root word\n","\n","        if c_node[\"head\"] == p_node[\"address\"]:\n","            return c_node[\"rel\"]\n","        else:\n","            return None\n","\n","    def _convert_to_binary_features(self, features):\n","        \"\"\"\n","        :param features: list of feature string which is needed to convert to binary features\n","        :type features: list(str)\n","        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n","        \"\"\"\n","        unsorted_result = []\n","        for feature in features:\n","            self._dictionary.setdefault(feature, len(self._dictionary))\n","            unsorted_result.append(self._dictionary[feature])\n","\n","        # Default value of each feature is 1.0\n","        return \" \".join(\n","            str(featureID) + \":1.0\" for featureID in sorted(unsorted_result)\n","        )\n","\n","    def _is_projective(self, depgraph):\n","        arc_list = []\n","        for key in depgraph.nodes:\n","            node = depgraph.nodes[key]\n","\n","            if \"head\" in node:\n","                childIdx = node[\"address\"]\n","                parentIdx = node[\"head\"]\n","                if parentIdx is not None:\n","                    arc_list.append((parentIdx, childIdx))\n","\n","        for (parentIdx, childIdx) in arc_list:\n","            # Ensure that childIdx < parentIdx\n","            if childIdx > parentIdx:\n","                temp = childIdx\n","                childIdx = parentIdx\n","                parentIdx = temp\n","            for k in range(childIdx + 1, parentIdx):\n","                for m in range(len(depgraph.nodes)):\n","                    if (m < childIdx) or (m > parentIdx):\n","                        if (k, m) in arc_list:\n","                            return False\n","                        if (m, k) in arc_list:\n","                            return False\n","        return True\n","\n","    def _write_to_file(self, key, binary_features, input_file):\n","        \"\"\"\n","        write the binary features to input file and update the transition dictionary\n","        \"\"\"\n","        self._transition.setdefault(key, len(self._transition) + 1)\n","        self._match_transition[self._transition[key]] = key\n","\n","        input_str = str(self._transition[key]) + \" \" + binary_features + \"\\n\"\n","        input_file.write(input_str.encode(\"utf-8\"))\n","\n","    #THE ARGUMENTS ARE CHANGED\n","    def _create_training_examples_arc_std(self, depgraphs, input_file, modified_features=False):\n","        \"\"\"\n","        Create the training example in the libsvm format and write it to the input_file.\n","        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n","        \"\"\"\n","        operation = Transition(self.ARC_STANDARD)\n","        count_proj = 0\n","        training_seq = []\n","\n","        for depgraph in depgraphs:\n","            if not self._is_projective(depgraph):\n","                continue\n","\n","            count_proj += 1\n","            conf = Configuration(depgraph)\n","            while len(conf.buffer) > 0:\n","                b0 = conf.buffer[0]\n","                features = conf.extract_features(modified_features)\n","                binary_features = self._convert_to_binary_features(features)\n","\n","                if len(conf.stack) > 0:\n","                    s0 = conf.stack[len(conf.stack) - 1]\n","                    # Left-arc operation\n","                    rel = self._get_dep_relation(b0, s0, depgraph)\n","                    if rel is not None:\n","                        key = Transition.LEFT_ARC + \":\" + rel\n","                        self._write_to_file(key, binary_features, input_file)\n","                        operation.left_arc(conf, rel)\n","                        training_seq.append(key)\n","                        continue\n","\n","                    # Right-arc operation\n","                    rel = self._get_dep_relation(s0, b0, depgraph)\n","                    if rel is not None:\n","                        precondition = True\n","                        # Get the max-index of buffer\n","                        maxID = conf._max_address\n","\n","                        for w in range(maxID + 1):\n","                            if w != b0:\n","                                relw = self._get_dep_relation(b0, w, depgraph)\n","                                if relw is not None:\n","                                    if (b0, relw, w) not in conf.arcs:\n","                                        precondition = False\n","\n","                        if precondition:\n","                            key = Transition.RIGHT_ARC + \":\" + rel\n","                            self._write_to_file(key, binary_features, input_file)\n","                            operation.right_arc(conf, rel)\n","                            training_seq.append(key)\n","                            continue\n","\n","                # Shift operation as the default\n","                key = Transition.SHIFT\n","                self._write_to_file(key, binary_features, input_file)\n","                operation.shift(conf)\n","                training_seq.append(key)\n","\n","        #print(\" Number of training examples : \" + str(len(depgraphs)))\n","        #print(\" Number of valid (projective) examples : \" + str(count_proj))\n","        return training_seq\n","\n","\n","    #THE ARGUMENTS ARE CHANGED\n","    def _create_training_examples_arc_eager(self, depgraphs, input_file, modified_features = False):\n","        \"\"\"\n","        Create the training example in the libsvm format and write it to the input_file.\n","        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n","        \"\"\"\n","        operation = Transition(self.ARC_EAGER)\n","        countProj = 0\n","        training_seq = []\n","\n","        for depgraph in depgraphs:\n","            if not self._is_projective(depgraph):\n","                continue\n","\n","            countProj += 1\n","            conf = Configuration(depgraph)\n","            while len(conf.buffer) > 0:\n","                b0 = conf.buffer[0]\n","                features = conf.extract_features(modified_features)\n","                binary_features = self._convert_to_binary_features(features)\n","\n","                if len(conf.stack) > 0:\n","                    s0 = conf.stack[len(conf.stack) - 1]\n","                    # Left-arc operation\n","                    rel = self._get_dep_relation(b0, s0, depgraph)\n","                    if rel is not None:\n","                        key = Transition.LEFT_ARC + \":\" + rel\n","                        self._write_to_file(key, binary_features, input_file)\n","                        operation.left_arc(conf, rel)\n","                        training_seq.append(key)\n","                        continue\n","\n","                    # Right-arc operation\n","                    rel = self._get_dep_relation(s0, b0, depgraph)\n","                    if rel is not None:\n","                        key = Transition.RIGHT_ARC + \":\" + rel\n","                        self._write_to_file(key, binary_features, input_file)\n","                        operation.right_arc(conf, rel)\n","                        training_seq.append(key)\n","                        continue\n","\n","                    # reduce operation\n","                    flag = False\n","                    for k in range(s0):\n","                        if self._get_dep_relation(k, b0, depgraph) is not None:\n","                            flag = True\n","                        if self._get_dep_relation(b0, k, depgraph) is not None:\n","                            flag = True\n","                    if flag:\n","                        key = Transition.REDUCE\n","                        self._write_to_file(key, binary_features, input_file)\n","                        operation.reduce(conf)\n","                        training_seq.append(key)\n","                        continue\n","\n","                # Shift operation as the default\n","                key = Transition.SHIFT\n","                self._write_to_file(key, binary_features, input_file)\n","                operation.shift(conf)\n","                training_seq.append(key)\n","\n","        return training_seq\n","\n","    #Additional parameter: \"spec_model\" describing the model of interest\n","    def train(self, depgraphs, modelfile, verbose=True, modified_features = False, spec_mod=\"svm\"):\n","        \"\"\"\n","        :param depgraphs : list of DependencyGraph as the training data\n","        :type depgraphs : DependencyGraph\n","        :param modelfile : file name to save the trained model\n","        :type modelfile : str\n","        \"\"\"\n","\n","        try:\n","            input_file = tempfile.NamedTemporaryFile(\n","                prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n","            )\n","\n","            if self._algorithm == self.ARC_STANDARD:\n","                self._create_training_examples_arc_std(depgraphs, input_file, modified_features)\n","            else:\n","                self._create_training_examples_arc_eager(depgraphs, input_file, modified_features)\n","\n","            input_file.close()\n","            \n","            # Using the temporary file to train the libsvm classifier\n","            x_train, y_train = load_svmlight_file(input_file.name)\n","\n","\n","            \n","            if (spec_mod == 'DecisionTreeClassifier'):\n","              print('dt')\n","              model = DecisionTreeClassifier()\n","              #the default criterion ('gini') [accuracy=0.89]\n","              #works better than the ('entropy') criterion [accuracy=0.87] \n","            \n","            elif (spec_mod == 'KNeighborsClassifier'):\n","              print('knn')\n","              from sklearn.neighbors import KNeighborsClassifier\n","              model = KNeighborsClassifier(n_neighbors=3)\n","\n","            elif (spec_mod == 'RandomForestClassifier'):\n","              print('rf')\n","              model = RandomForestClassifier(random_state=0)\n","\n","            elif (spec_mod == 'MLPClassifier'):\n","              print('mlp')\n","              #AGGIUNTA MIA - MLP\n","              from sklearn.neural_network import MLPClassifier\n","              model = MLPClassifier(random_state=4, max_iter=300)\n","\n","            else:\n","              print('svm')\n","              # The parameter is set according to the paper:\n","              # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n","              # Todo : because of probability = True => very slow due to\n","              # cross-validation. Need to improve the speed here\n","              model = svm.SVC(\n","                kernel=\"poly\",\n","                degree=2,\n","                coef0=0,\n","                gamma=0.2,\n","                C=0.5,\n","                verbose=verbose,\n","                probability=True,\n","                )\n","              \n","            model = model.fit(x_train, y_train)\n","\n","\n","            #FINE AGGIUNTA MIA\n","\n","            # Save the model to file name (as pickle)\n","            pickle.dump(model, open(modelfile, \"wb\"))\n","        \n","        \n","        finally:\n","            remove(input_file.name)\n","\n","\n","    def parse(self, depgraphs, modelFile, modified_features=False):\n","        \"\"\"\n","        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n","        :type depgraphs: list(DependencyGraph)\n","        :param modelfile: the model file\n","        :type modelfile: str\n","        :return: list (DependencyGraph) with the 'head' and 'rel' information\n","        \"\"\"\n","        result = []\n","        # First load the model\n","        model = pickle.load(open(modelFile, \"rb\"))\n","        operation = Transition(self._algorithm)\n","\n","        for depgraph in depgraphs:\n","            conf = Configuration(depgraph)\n","            while len(conf.buffer) > 0:\n","                features = conf.extract_features(modified_features)\n","                col = []\n","                row = []\n","                data = []\n","                for feature in features:\n","                    if feature in self._dictionary:\n","                        col.append(self._dictionary[feature])\n","                        row.append(0)\n","                        data.append(1.0)\n","                np_col = array(sorted(col))  # NB : index must be sorted\n","                np_row = array(row)\n","                np_data = array(data)\n","\n","                x_test = sparse.csr_matrix(\n","                    (np_data, (np_row, np_col)), shape=(1, len(self._dictionary))\n","                )\n","\n","\n","                prob_dict = {}\n","                pred_prob = model.predict_proba(x_test)[0]\n","                for i in range(len(pred_prob)):\n","                    prob_dict[i] = pred_prob[i]\n","                sorted_Prob = sorted(prob_dict.items(), key=itemgetter(1), reverse=True)\n","\n","                # Note that SHIFT is always a valid operation\n","                for (y_pred_idx, confidence) in sorted_Prob:\n","                    # y_pred = model.predict(x_test)[0]\n","                    # From the prediction match to the operation\n","                    y_pred = model.classes_[y_pred_idx]\n","\n","                    if y_pred in self._match_transition:\n","                        strTransition = self._match_transition[y_pred]\n","                        baseTransition = strTransition.split(\":\")[0]\n","\n","                        if baseTransition == Transition.LEFT_ARC:\n","                            if (\n","                                operation.left_arc(conf, strTransition.split(\":\")[1])\n","                                != -1\n","                            ):\n","                                break\n","                        elif baseTransition == Transition.RIGHT_ARC:\n","                            if (\n","                                operation.right_arc(conf, strTransition.split(\":\")[1])\n","                                != -1\n","                            ):\n","                                break\n","                        elif baseTransition == Transition.REDUCE:\n","                            if operation.reduce(conf) != -1:\n","                                break\n","                        elif baseTransition == Transition.SHIFT:\n","                            if operation.shift(conf) != -1:\n","                                break\n","                    else:\n","                        raise ValueError(\n","                            \"The predicted transition is not recognized, expected errors\"\n","                        )\n","\n","            # Finish with operations build the dependency graph from Conf.arcs\n","\n","            new_depgraph = deepcopy(depgraph)\n","            for key in new_depgraph.nodes:\n","                node = new_depgraph.nodes[key]\n","                node[\"rel\"] = \"\"\n","                # With the default, all the token depend on the Root\n","                node[\"head\"] = 0\n","            for (head, rel, child) in conf.arcs:\n","                c_node = new_depgraph.nodes[child]\n","                c_node[\"head\"] = head\n","                c_node[\"rel\"] = rel\n","            result.append(new_depgraph)\n","\n","        return result\n"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9myDAMzU5mCt","executionInfo":{"status":"ok","timestamp":1618884729327,"user_tz":-120,"elapsed":429752,"user":{"displayName":"Marco De Luca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4bvlddv9S2r5WXxd3HMPzGFo0gE5n6qyWYHYpNg=s64","userId":"11570652135950771421"}},"outputId":"39a4998c-ad5a-4d25-b1ee-4e7788e10144"},"source":["!pip install pytextdist\n","from nltk.parse.dependencygraph import DependencyGraph\n","from nltk.parse import ProbabilisticProjectiveDependencyParser\n","from nltk.corpus import dependency_treebank\n","import nltk\n","from nltk.parse import DependencyEvaluator\n","\n","nltk.download('dependency_treebank')\n","\n","# print dependency graph in CoNLL format\n","#print(dependency_treebank.parsed_sents()[0].to_conll(10))\n","\n","ppdp = ProbabilisticProjectiveDependencyParser()\n","\n","# train parser on graphs\n","#ppdp.train(dependency_treebank.parsed_sents())\n","\n","tp = TransitionParser('arc-standard')\n","\n","modified_features = True\n","\n","#SVM \n","res = tp.train(dependency_treebank.parsed_sents()[:500], 'tp.model', modified_features=modified_features, spec_mod = 'svm')\n","#equivalent to the default value\n","#res = tp.train(dependency_treebank.parsed_sents()[:500], 'tp.model', modified_features=modified_features)\n","#NB: Any string passed to spec_mod which is not part of the ones listed before, will make the model execute the 'svm' model\n","\n","#Decision Tree Classifier\n","#res = tp.train(dependency_treebank.parsed_sents()[:3500], 'tp.model', modified_features=modified_features, spec_mod = 'DecisionTreeClassifier')\n","\n","#Random Forest\n","#res =tp.train(dependency_treebank.parsed_sents()[:500], 'tp.model', modified_features=modified_features, spec_mod = 'RandomForestClassifier')\n","\n","#Multi-Layer Perceptron\n","#res = tp.train(dependency_treebank.parsed_sents()[:500], 'tp.model', modified_features=modified_features, spec_mod = 'MLPClassifier')\n","\n","# parsing takes a list of dependency graphs and a model as arguments\n","parses = tp.parse(dependency_treebank.parsed_sents()[-50:], 'tp.model', modified_features=modified_features)\n","de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-50:])\n","las, uas = de.eval()\n","print(las)\n","print(uas)\n"],"execution_count":75,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytextdist in /usr/local/lib/python3.7/dist-packages (0.1.6)\n","[nltk_data] Downloading package dependency_treebank to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package dependency_treebank is already up-to-date!\n","svm\n","[LibSVM]0.9039408866995073\n","0.9039408866995073\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sGucpJuO2h45"},"source":[""],"execution_count":null,"outputs":[]}]}